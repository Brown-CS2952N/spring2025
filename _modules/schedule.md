---
title: Paper reading list and presenters
---

**The schedule is work in progress.**

Jan 31, Tue
: [Course Overview](https://drive.google.com/file/d/17ykUcxQlraQLcQMIdlgUJMfBBF0ZieMN/view?usp=sharing) ([Slides](https://docs.google.com/presentation/d/1VSeMBMoJypRIVydBp8Q7DdAg_M1ZNinD/edit?usp=sharing&ouid=107174457905760692415&rtpof=true&sd=true))
  : Chen Sun
: 1. (Background) [How to Read a CS Research Paper](http://www2.cs.uregina.ca/~pwlfong/CS499/reading-paper.pdf) by Philip Fong
  1. (Background) [How to do research](http://people.csail.mit.edu/billf/publications/How_To_Do_Research.pdf) by Bill Freeman
  1. (Background) [How to do write a good paper](https://billf.mit.edu/sites/default/files/documents/cvprPapers.pdf) by Bill Freeman
  1. (Background) [How to speak (video)](https://www.youtube.com/watch?v=Unzc731iCUY) by Patrick Winston


Feb. 2, Thu
: [Deep Learning Recap](https://drive.google.com/file/d/1UFBSsH8lt56oL7iG2NHrRwnXRVzHg8pG/view?usp=share_link) ([Slides](https://docs.google.com/presentation/d/1kmvUO4ebot_Mz7-IbH3SspvZZgJN1x3m/edit?usp=sharing&ouid=107174457905760692415&rtpof=true&sd=true))
  : Chen Sun
: 1. (Background) [Novelty in Science](https://perceiving-systems.blog/en/news/novelty-in-science) by Michael Black
  1. (Background) [Everything is Connected: Graph Neural Networks](https://arxiv.org/abs/2301.08210)


Feb. 6, Mon
: **Due**{: .label .label-purple} [Presentation signup sheet](https://forms.gle/4CnTJGqKKx3vMWr89)


Feb. 7, Tue
: [Learning with Various Supervision](https://drive.google.com/file/d/1rbTVV1EtDcQEkauEazafymQxgpmq5puK/view?usp=sharing) ([Slides](https://drive.google.com/file/d/1nTfL79Sx_taukBb_Txh727HDGLcnW6eV/view?usp=sharing))
  : Chen Sun
: 1. (Background) [How to grow a mind: Statistics, structure, and abstraction](https://wiki.santafe.edu/images/e/e1/HowToGrowAMind%282011%29Tenebaum_J.pdf)
  1. (Background) [ICLR Debate with Leslie Kaelbling (video)](https://www.youtube.com/watch?v=veG8S5rqKIE)
  1. (Background) Learning with not Enough Data by Lilian Weng ([part1](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/) / [part2](https://lilianweng.github.io/posts/2022-02-20-active-learning/))


Feb. 9, Thu
: [The Bitter Lesson](https://drive.google.com/file/d/1clMnHW9TAhvDdlCGBZILE0dIDdZRAbqA/view?usp=share_link) ([Reading survey](https://forms.gle/LodhW8oy1tM4YJ2V8) / [Slides](https://docs.google.com/presentation/d/1z-gv9VkRKuGJcPYTuVxy0ybeT3PppoJaoowmsy-jx0c/edit?usp=sharing))
  : Amina, Ilija, and Raymond
: 1. [Revisiting Unreasonable Effectiveness of Data in the Deep Learning Era](https://arxiv.org/abs/1707.02968)
  1. [Unbiased Look at Dataset Bias](https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf)
  1. (Background) [The bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
  1. (Background) [The Unreasonable Effectiveness of Data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)
  1. (Background) [Exploring Randomly Wired Neural Networks for Image Recognition](https://arxiv.org/abs/1904.01569)
  1. (Background) [NAS evaluation is frustratingly hard](https://arxiv.org/abs/1912.12522)


Feb. 14, Tue
: [Semi-supervised Learning](https://drive.google.com/file/d/1RQL2hrCI9wYZ_n_BRJYpGmmyWxz4lQGd/view?usp=sharing) ([Reading survey](https://forms.gle/S8hdDzNAcZ5FgfTb7) / [Slides](https://docs.google.com/presentation/d/1R-y6WaBqKcEdMTBaw_tTHdE3s-JCcJ0Wr1N0Tom6WgI/edit?usp=sharing))
  : Rosella, Patrick, Lingyu, and Michael
: 1. [Mean teachers are better role models](https://arxiv.org/abs/1703.01780)
  1. [MixMatch: A Holistic Approach to Semi-Supervised Learning](https://arxiv.org/abs/1905.02249)
  1. (Presentation) [Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning](https://arxiv.org/pdf/1704.03976.pdf)
  1. (Presentation) [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/pdf/2001.07685.pdf)
  1. (Background) [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)
  1. (Background) [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)
  1. (Background) [Transfer Learning in a Transductive Setting](https://papers.nips.cc/paper/2013/hash/3295c76acbf4caaed33c36b1b5fc2cb1-Abstract.html)


Feb. 16, Thu
: [Transfer Learning](https://drive.google.com/file/d/1hmNifPJAukPlHKH-hkNP8VYdUnwD5T1l/view?usp=sharing) ([Reading survey](https://forms.gle/2e3ab7oAUYqaYfsV9) / [Slides](https://docs.google.com/presentation/d/1o3dH_7fgqhQ4MC19UbHLQu9Px6AvrtGwzq0VsPpURWQ/edit?usp=sharing))
  : Wasiwasi, Abubakarr, Yiqing, and Jacob
: 1. [Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks](https://leon.bottou.org/publications/pdf/cvpr-2014.pdf)
  1. [Transfusion: Understanding Transfer Learning for Medical Imaging](https://arxiv.org/abs/1902.07208)
  1. (Background) [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)
  1. (Background) [Rethinking Pre-training and Self-training](https://arxiv.org/abs/2006.06882)
  1. (Background) [A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark](https://arxiv.org/abs/1910.04867)
  1. (Background) [Rethinking ImageNet Pre-training](https://arxiv.org/abs/1811.08883)
  1. (Background) [Natural Language Processing (almost) from Scratch](https://arxiv.org/abs/1103.0398)


Feb. 21, Tue
: _University holiday, no class_


Feb. 23, Thu
: [Few-shot and In-context Learning](https://drive.google.com/file/d/1X3Vr_1XWXr7Pa2-wGweWrA-PWE3Gfyqt/view?usp=sharing) ([Reading survey](https://forms.gle/H9vegd9ya1NLVLHGA) / [Slides](https://docs.google.com/presentation/d/1uYar7IViqk4b2s7XXgkjcVh1HIYDHfXb47mCNkEum5k/edit?usp=sharing))
  : Sheridan, Shreyas, and Zhuo
: 1. [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080)
  1. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  1. (Background) [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
  1. (Background) [Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)
  1. (Background) [Learning to Learn (Blog)](https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)
  1. (Background) [Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?](https://arxiv.org/abs/2003.11539)
  1. (Background) [Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf)
  1. (Background) [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)


Feb. 28, Tue
: [Multitask Learning](https://drive.google.com/file/d/1JkkhR7ENO0XTr-1U_3nr4Swz6fbPMEgD/view?usp=sharing) ([Reading survey](https://forms.gle/jqKrGfCNPCwbkhy49) / [Slides](https://docs.google.com/presentation/d/1wv9omE95rlC6HmfBSWrWPboPKfrzi1TZubbNTaH9dmQ/edit?usp=sharing))
  : Noah, Alexander, Pinyuan
: 1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (Section 1, 2, 4)
  1. [A Generalist Agent](https://arxiv.org/abs/2205.06175)
  1. (Background) [Intelligence without representation](https://people.csail.mit.edu/brooks/papers/representation.pdf)
  1. (Background) [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
  1. (Background) [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/abs/1804.08328)
  1. (Background) [UberNet: Training a Universal Convolutional Neural Network](https://arxiv.org/abs/1609.02132)
  1. (Background) [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](https://arxiv.org/abs/2206.08916)

  
Mar. 2, Thu
: [Transformer and its variants](https://drive.google.com/file/d/1I85ILWzNErAyeS0Lq3hhJ6xN4sOXvO41/view?usp=share_link) ([Reading survey](https://forms.gle/NsTjYZjNY4axTULLA) / [Slides](https://docs.google.com/presentation/d/1uFDwi6rSTw_tHwA8Rd8cNplEcIjVBNKlwu2hydsRVRs/edit?usp=share_link))
  : Daniel, Yuan, and David
: 1. [Swin Transformer](https://arxiv.org/abs/2103.14030)
  1. [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)
  1. (Background) [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
  1. (Background) [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)
  1. (Background) [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
  1. (Background) [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
  1. (Background) [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)  


Mar. 7, Tue
: [Self-supervised and Multimodal Learning](https://drive.google.com/file/d/1tZZwwDBPz6cywkDa4AoPzRdFyjZq9Y-0/view?usp=sharing) ([Slides](https://drive.google.com/file/d/1b2Fk0I1LujvnnURi7yJryEXOFspEvaI0/view?usp=sharing))
  : Chen Sun
: 1. (Background) [Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/) by Lilian Weng
  1. (Background) [Contrastive Representation Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/) by Lilian Weng
  1. (Background) [Self-Supervised Learning](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf) by Andrew Zisserman


Mar. 9, Thu
: [Self-supervised Learning for NLP](https://drive.google.com/file/d/1NaWT42c8LGIdV5i6z0dy7N0zqXsD6fC0/view?usp=share_link) ([Reading survey](https://forms.gle/F3ggE2P6VHD63Aan7) / [Slides](https://docs.google.com/presentation/d/1sfdZX_MIfdbVQcPwDDH0KbFZ9UgfVqhPWyrLDl0SYj0/edit?usp=sharing))
  : Yang, Adrian, and Vignesh
: 1. [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
  1. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827)
  1. (Background) [Self Supervision Does Not Help Natural Language Supervision at Scale](https://arxiv.org/abs/2301.07836)
  1. (Background) [How does in-context learning work?](https://ai.stanford.edu/blog/understanding-incontext/)
  1. (Background) [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529)
  1. (Background) [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
  1. (Background) [Human Language Understanding & Reasoning](https://www.amacad.org/publication/human-language-understanding-reasoning)
  1. (Background) [Do Large Language Models Understand Us?](https://www.amacad.org/publication/do-large-language-models-understand-us)


Mar. 9, Thu
: **Project**{: .label .label-purple} [Final project signup](https://forms.gle/8jJZN6UvavBi4enC9)
  : Due on 3/16

Mar. 10, Fri
: **Homework**{: .label .label-purple} [First mini project](https://docs.google.com/document/d/1A87spJ2DxF6F0gRqyQp96YZeSw2kaXlhJ9YCoCELApI/edit?usp=sharing)
  : Due on 4/28


Mar. 14, Tue
: **Invited**{: .label .label-purple} [Computer Vision for Global Scale Biodiversity Monitoring](https://drive.google.com/file/d/1od3-w7Tn_QjeKsWeSCd3JHVtg1OLt25E/view?usp=sharing)
  : [Sara Beery](https://beerys.github.io/)


Mar. 16, Thu
: [Self-supervised Learning for Images and Videos](https://drive.google.com/file/d/1IbIwFR_8aULBc30msmk0Plod3h6gU4RA/view?usp=sharing) ([Reading survey](https://forms.gle/Xq6WKQ8uFaQvDiVn7) / [Slides](https://docs.google.com/presentation/d/1glMf_5P5qjeAEwgl4KV65XfdHtrjoWE-HXHCL9zdoTw/edit?usp=sharing))
  : Arthur, Robert, and Siyang
: 1. [Dimensionality Reduction by Learning an Invariant Mapping](https://ieeexplore.ieee.org/document/1640964)
  1. [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
  1. (Background) [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)
  1. (Background) [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)
  1. (Background) [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
  1. (Background) [Deep Clustering for Unsupervised Learning of Visual Features](https://arxiv.org/abs/1807.05520)
  1. (Background) [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
  1. (Background) [Learning image representations tied to ego-motion](https://arxiv.org/abs/1505.02206)


Mar. 21, Tue
: **Invited**{: .label .label-purple} [The Future of Computer Vision via Foundation Models and Beyond](https://drive.google.com/file/d/1lZEvPcNX46XAaLvG8m-TDy6QSBZ8NI3d/view?usp=sharing)
  : [Ce Liu](http://people.csail.mit.edu/celiu/)


Mar. 23, Thu
: [Project proposal](https://drive.google.com/file/d/1ElitXltMY0RsSXpwKH2IbvU4LnMTroAp/view?usp=sharing) ([Slides](https://docs.google.com/presentation/d/1zRjCtqidvMvogfbcc7yM3h14Nz7JC8yX4Cnul67oTwQ/edit?usp=sharing))

Mar. 23, Thu
: **Feedback**{: .label .label-purple} [Mid-semester Feedback Form](https://forms.gle/AtsxomKRQRAQ3WTf6)

Mar. 28, Tue
: **Homework**{: .label .label-purple} [Second mini project](https://docs.google.com/document/d/13GjqihWU1myGRaO0yKloqusF4Ehf1LYYhc7-pLXZIN0/edit?usp=sharing)
  : Due on 4/28

Mar. 28, Tue
: _Spring break_

Mar. 30, Thu
: _Spring break_

Apr. 4, Tue
: [Reinforcement Learning](https://drive.google.com/file/d/1ZNWQjVc9oHrXbROMATNJT7l3lUwrMN8v/view?usp=sharing) ([Slides](https://drive.google.com/file/d/1_wpikU3ccspsBDCjAzv0XjVLPMhEBI1V/view?usp=sharing))
  : Chen Sun

Apr. 6, Thu
: [World Models](https://drive.google.com/file/d/1bq_-n4jT8qU552Zv-snTCIbdnxTF6Tbd/view?usp=sharing) ([Reading survey](https://forms.gle/A9DhHVFK5o5YweWL9) / [Slides](https://docs.google.com/presentation/d/1RQkEAf92dZbh6ANHM-1RrCAkJd23Uclit7bgTo5pYss/edit?usp=sharing))
  : Ray, Alexander, and Paul
: 1. [World Models](https://arxiv.org/abs/1803.10122)
  1. [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551)
  1. (Background) [Mastering Diverse Domains through World Models](https://arxiv.org/abs/2301.04104v1)
  1. (Background) [Control-Aware Representations for Model-based Reinforcement Learning](https://arxiv.org/abs/2006.13408)
  1. (Background) [Shaping Belief States with Generative Environment Models for RL](https://arxiv.org/abs/1906.09237)
  1. (Background) [Model-Based Reinforcement Learning: Theory and Practice](https://bair.berkeley.edu/blog/2019/12/12/mbpo/)
  1. (Background) [DayDreamer: World Models for Physical Robot Learning](https://arxiv.org/abs/2206.14176)

Apr. 11, Tue
: [Generative Models](https://drive.google.com/file/d/12ddNn7tT7DimISVlkZC6RNy77Uo2mBq2/view?usp=sharing) ([Slides](https://docs.google.com/presentation/d/1xP4zi5goYdMfu5TLY01Q00yJbXmIIHp1CDNm3gqFmtU/edit?usp=sharing))
  : Calvin Luo
: 1. (Background) [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)

Apr. 13, Thu
: [RL from Human Feedback](https://drive.google.com/file/d/17xTpwxK0yCkA0dL0pdzQMqFgWUupZD5o/view?usp=share_link) ([Reading survey](https://forms.gle/ofmkoVK7tGB47wBfA) / [Slides](https://docs.google.com/presentation/d/12Zb5M3N9U-iDTQ9uNbAoZQmlrpk3jzwSlViZnPYwW4A/edit?usp=share_link))
  : Ziyi, Qi, and Christopher
: 1. [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)
  1. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
  1. (Background) [Why does ChatGPT constantly lie?](https://noahpinion.substack.com/p/4e262415-6b0e-41b7-ba2d-8f620790bf63) by Noah Smith
  1. (Background) [ChatGPT Is a Blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) by Ted Chiang
  1. (Background) [Stanford CS224N](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf)
  1. (Background) [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)
  1. (Background) [Reinforcement Learning for Language Models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)

Apr. 18, Tue
: [Learning from Offline Demonstration](https://drive.google.com/file/d/1ne7Xw1M4HtQgwe64WEFUnCBnLgqifVPh/view?usp=sharing) ([Reading survey](https://forms.gle/AfJFinqdpWksPRKn9) / [Slides](https://docs.google.com/presentation/d/1R84mqEEH6VkjeSnY3BfypMLDCWSGSewwWcKva-erGcw/edit?usp=sharing))
  : Anirudha, Zilai, and Akash
: 1. [Learning to Act by Watching Unlabeled Online Videos](https://cdn.openai.com/vpt/Paper.pdf)
  1. [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)
  1. (Background) [Language Conditioned Imitation Learning over Unstructured Data](https://arxiv.org/abs/2005.07648)
  1. (Background) [Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://minedojo.org/)
  1. (Background) [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
  1. (Background) [Understanding the World Through Action](https://arxiv.org/abs/2110.12543)
  1. (Background) [Learning Latent Plans from Play](https://arxiv.org/abs/1903.01973)

Apr. 20, Thu
: [3D Generation](https://drive.google.com/file/d/16ZNhl-7VTBB45RfbtVHE6-b20BVp9deo/view?usp=share_link) ([Reading survey](https://forms.gle/F3aEX7FRBVcncNjg9) / [Slides](https://docs.google.com/presentation/d/1DsqEoZyHKvBFFnzFG4YvZxM9XqT_HbvTcd9kHqYX1yc/edit?usp=sharing))
  : Nitya, Linghai, and Yuan
: 1. [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988)
  1. [RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation](https://arxiv.org/abs/2211.09869)
  1. (Background) [Equivariant Diffusion for Molecule Generation in 3D](https://proceedings.mlr.press/v162/hoogeboom22a/hoogeboom22a.pdf)
  1. (Background) [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751)
  1. (Background) [Text-To-4D Dynamic Scene Generation](https://arxiv.org/abs/2301.11280)
  1. (Background) [Zero-Shot Text-Guided Object Generation with Dream Fields](https://arxiv.org/abs/2112.01455)
 

Apr. 25, Tue
: [Compositionality](https://drive.google.com/file/d/17MoqGd3sRAJtHRGIn7J75aFC0JwYMJeg/view?usp=sharing) ([Reading survey](https://forms.gle/JnJqHwcv5zbosJ2HA) / [Slides](https://docs.google.com/presentation/d/1SYvLpB5u6Ulg6ammSOmS2T2HDqCrpT3fA1CQ4-YLaLI/edit?usp=sharing))
  : Lingze, Suraj, and Apoorv
: 1. [Learning to Compose Neural Networks for Question Answering](https://arxiv.org/abs/1601.01705)
  1. [Compositional Visual Generation with Composable Diffusion Models](https://arxiv.org/abs/2206.01714)
  1. (Background) [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)
  1. (Background) [CREPE: Can Vision-Language Foundation Models Reason Compositionally?](https://arxiv.org/abs/2212.07796)
  1. (Background) [COGS: A Compositional Generalization Challenge Based on Semantic Interpretation](https://aclanthology.org/2020.emnlp-main.731/)
  1. (Background) [Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems](https://arxiv.org/abs/2205.01128)

Apr. 27, Thu
: [Model Interpretability](https://drive.google.com/file/d/1_lIFYM9UPSBsJ-Ita9MX240E9oPGiJjG/view?usp=sharing) ([Reading survey](https://forms.gle/PdokHxodf2YKYVwA8) / [Slides](https://docs.google.com/presentation/d/1lwZb3TjUy_lL0TIYCt2db9BtHig62UYnUGDDDeqNa74/edit?usp=sharing))
  : Michael, Haoyu, and Qinan
: 1. [Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810)
  1. [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)
  1. (Background) [BERT rediscovers the classical NLP pipeline](https://arxiv.org/abs/1905.05950)
  1. (Background) [Concept Bottleneck Models](https://arxiv.org/abs/2007.04612)
  1. (Background) [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062)
  1. (Background) [Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312)
  1. (Background) [Neural Networks and the Chomsky Hierarchy](https://arxiv.org/abs/2207.02098)


Apr. 28, Fri
: **Due**{: .label .label-purple} Last day to submit mini projects

May 2, Tue
: _Final project office hours_

May 4, Thu
: _Final project office hours_

May 11, Thu
: [Final project presentations](https://drive.google.com/file/d/1ysNoekES4WrEPz_vFRMdwbXN0ABRPbjF/view?usp=sharing) (CIT 368, noon to 2:30pm) ([Slides](https://docs.google.com/presentation/d/1aBnWRZvkMeLrAYfmgubpt2iRMyfeG91vnqhTf20oGuk/edit?usp=sharing))

May 12, Fri
: **Due**{: .label .label-purple} Project submission ([Form](https://forms.gle/CuQUWSb8XE5uZUZy9))
